# Ollama AI/ML Workload Profile
# Optimized for local LLM inference with GPU acceleration

project = "ollama-ai-workload"

[profiles.ollama-inference]
gpu.nvidia.cuda = true
gpu.nvidia.power_limit = 110
memory.huge_pages = true
cpu.governor = "performance"
network.priority = "background"

[services.ollama]
image = "ollama/ollama:latest"
ports = ["11434:11434"]
volumes = [
    "ollama_models:/root/.ollama",
    "/dev/nvidia0:/dev/nvidia0",
    "/dev/nvidiactl:/dev/nvidiactl",
    "/dev/nvidia-modeset:/dev/nvidia-modeset",
    "/dev/nvidia-uvm:/dev/nvidia-uvm",
    "/dev/nvidia-uvm-tools:/dev/nvidia-uvm-tools"
]
environment = {
    "NVIDIA_VISIBLE_DEVICES" = "all",
    "NVIDIA_DRIVER_CAPABILITIES" = "compute,utility",
    "OLLAMA_GPU_LAYERS" = "35",
    "OLLAMA_MAX_LOADED_MODELS" = "2",
    "OLLAMA_FLASH_ATTENTION" = "1"
}

[services.ollama.gaming]
[services.ollama.gaming.gpu]
passthrough = true

[services.ollama.gaming.gpu.nvidia]
device = 0
cuda = true
power_limit = 110
memory_clock_offset = 0
core_clock_offset = 0

[services.ollama.gaming.performance]
cpu_governor = "performance"
nice_level = 0
rt_priority = 1

# Web UI for Ollama
[services.ollama-webui]
image = "ghcr.io/open-webui/open-webui:main"
ports = ["3000:8080"]
volumes = ["open_webui:/app/backend/data"]
environment = {
    "OLLAMA_BASE_URL" = "http://ollama:11434",
    "WEBUI_SECRET_KEY" = "your-secret-key"
}
depends_on = ["ollama"]

# Optional: Model downloader service
[services.model-downloader]
image = "ollama/ollama:latest"
volumes = ["ollama_models:/root/.ollama"]
environment = {
    "OLLAMA_BASE_URL" = "http://ollama:11434"
}
depends_on = ["ollama"]

[volumes.ollama_models]
driver = "local"

[volumes.open_webui]
driver = "local"

[networks.ai]
driver = "bolt"
subnet = "10.7.0.0/16"